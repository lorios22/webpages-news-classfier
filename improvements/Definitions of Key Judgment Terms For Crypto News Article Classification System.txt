Definitions of Key Judgment Terms For Crypto News Article Classification System
Accurate
Definition: “Accurate” means free from errors and exactly truthful​
merriam-webster.com
. In content evaluation, an accurate article presents information that conforms to reality and verifiable facts. All names, dates, figures, and quotations are correct, and the context is properly conveyed. High accuracy implies not only that individual facts are correct, but also that they are presented in the right context so as not to mislead. For example, stating that “Bitcoin’s all-time high was about $69,000 in November 2021” is accurate if both the figure and date are correct. Accuracy includes being precise and careful: an accurate report uses the right statistics, correctly attributes statements to sources, and doesn’t omit critical qualifiers. In the realm of crypto articles, accuracy might mean correctly explaining technical terms and reporting blockchain data without mistakes. In macroeconomic content, accuracy entails using up-to-date economic indicators and factual historical data. In political articles, it means correctly quoting officials and citing laws or events without distortion.
Context and Nuance: Accuracy must be considered alongside context and intent. Sometimes a statement can be factually true but presented out of context in a way that misleads – in such cases it’s not fully accurate in spirit. Agents should distinguish between a minor factual slip (which still makes content inaccurate) versus a systemic issue of inaccuracy. An article that is accurate builds credibility, whereas inaccuracies undermine trust. Importantly, when evaluating accuracy, one should also consider relevance: an article can be accurate about outdated facts but if those facts are no longer relevant (e.g. using last year’s data for a rapidly changing market without noting the date), it may still mislead readers. Thus, accuracy works hand-in-hand with timeliness. Intent and bias can affect accuracy as well. Some inaccuracies are unintentional (errors or typos), while others might stem from bias (cherry-picking facts) or deliberate disinformation. Regardless of intent, any content that contains clear factual errors is inaccurate and should be treated cautiously. Even well-intended misinformation (unintentional) can cause harm if not corrected. Agents should verify key facts in crypto articles (like protocol details or market prices), macroeconomic pieces (like GDP figures or policy rates), and political stories (like election results or quotes) to judge accuracy. Accurate content forms the foundation for higher quality scores because without factual correctness, other positive qualities (depth, balance, etc.) lose value.
Balanced / Objective
Definition: “Balanced” (or objective) content is writing that presents information in a fair, impartial manner, covering multiple sides of an issue without unduly favoring one perspective. In other words, it’s content that is closely connected to facts rather than personal feelings or bias​
britannica.com
​
britannica.com
. An objective article maintains neutrality in tone and gives due weight to different viewpoints or pieces of evidence. For instance, a balanced political article discussing a policy would outline arguments both for and against the policy, supported by facts, instead of only praising or denouncing it. Balanced content acknowledges uncertainties and limitations instead of asserting one narrative as absolutely correct. In the context of macroeconomics, an objective piece might present both optimistic and pessimistic forecasts from different economists, letting the data speak for itself. In crypto journalism, being balanced could mean reporting both the potential of a new technology and its risks or criticisms, rather than acting as pure hype or pure doom-saying. The intent of balanced writing is usually to inform rather than persuade, which aligns with higher-quality intent.
Application and Significance: Balanced content is considered high-quality because it treats the subject matter fairly and respects the reader’s ability to form their own opinion based on a full picture. An article that is unbiased avoids loaded or emotional language and doesn’t hide inconvenient facts. This does not mean the writer cannot have any viewpoint, but a balanced article will still present evidence evenhandedly. Bias and balance are opposites: bias means one-sidedness (see Biased definition), whereas balance means fairness. Importantly, balanced content usually correlates with factual accuracy and completeness – by presenting all sides, the article likely includes more context (reducing cherry-picking). Images and charts can also reflect balance: e.g. a chart that compares two economic scenarios side-by-side without a misleading scale shows an objective use of data. An agent should reward content that remains neutral in its analysis and uses evidence rather than hyperbole, as it signals trustworthiness and comprehensive coverage. Balanced articles tend to score in the higher range (e.g. “Good” to “Outstanding”) because they provide the reader with a well-rounded understanding, increasing the article’s utility and credibility.
Biased
Definition: “Biased” content is the opposite of balanced. It exhibits a one-sided perspective, favoring a particular viewpoint, person, or group – often to the point of distorting the truth or omitting important information. Bias can be explicit or subtle, but it means the article is not impartial. A formal definition of bias is “a prejudice in favor of or against one thing, person, or group compared with another, usually in a way that’s considered unfair.”​
admin.smc.edu
In practice, a biased article might present only evidence supporting one side of an argument and ignore any counter-evidence. For example, a biased crypto article written by an avid promoter of a specific coin might highlight only that coin’s price gains and positive developments while ignoring any security issues or market risks. In politics, bias may show as a news piece that only criticizes one political party and never acknowledges any faults on the other side (or vice versa). Bias often creeps in through language too – using loaded words (like calling something “disastrous” or “miraculous” without factual support) signals a slant.
Context in Domains: In cryptocurrency content, common biases include maximalism (e.g., “only Bitcoin is good, everything else is a scam”) or promotional bias (where an author has a stake in a project and only shares its positives). In macroeconomics, bias might appear if an article written by someone with a strong ideological stance (say, pro-regulation vs. anti-regulation) only presents data favorable to their stance. In political articles, bias can be partisan (favoring one party/ideology), manifested by only quoting sources from one side or framing stories to make one side look exclusively good or bad. Intent and Impact: Bias can be intentional (propaganda or persuasion) or unintentional (the author’s own blind spots). Regardless, biased content is less reliable because it selectively filters reality. A biased article often correlates with cherry-picking of facts and may be misleading by omission. For instance, an economic analysis that touts a country’s growth but neglects to mention rising inequality is biased by omission of context, potentially misleading readers about overall well-being. Biased content, while not always containing outright falsehoods, skews the reader’s understanding. Therefore, agents should downgrade heavily biased pieces to lower quality scores (e.g., below average) unless balance is restored elsewhere in the content.
Detecting Bias: Agents should look for telltale signs of bias: one-sided sourcing, absence of alternative viewpoints, dismissive language toward opposing views, or consistent framing that advantages one narrative. Also consider source bias – sometimes the publication or author has a known agenda. For example, an article on a political topic from an openly partisan blog is likely biased by default. However, agents must focus on the content itself: occasionally a partisan source can still present facts fairly (though rare). Always check if the content acknowledges other perspectives or counterarguments. If it doesn’t, note that bias. Also, check intent: biased content might still be factually correct (no inaccuracies) yet incomplete and slanted, which means it shouldn’t score as highly as an objective, comprehensive piece. Bias reduces credibility and completeness, and agents use our definitions to identify it and adjust scores downward accordingly.
Cherry-Picking
Definition: “Cherry-picking” is the selective use of data or evidence that supports one argument while ignoring relevant information that would counter or provide a fuller picture​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j. In effect, the author cherry-picks when they present only certain facts to persuade the audience, withholding evidence that might contradict their claims​
logicallyfallacious.com
. This tactic results in a misleading impression: the data points shown might be true individually, but because contrary data is omitted, the overall conclusion drawn can be false or exaggerated. For example, imagine a crypto article bragging that “Token X surged 50% in the last week,” implying it’s a hot investment – but omitting that Token X had previously crashed by 90%. By cherry-picking the short-term surge and ignoring the longer history, the article gives an overly positive impression. Similarly, in a macroeconomic context, an analysis might highlight one favorable unemployment statistic from one month while ignoring a longer-term negative trend or other economic indicators, thereby skewing the reader’s perception. In politics, cherry-picking could involve quoting one favorable poll result for a candidate while ignoring several other polls that show the candidate trailing – presenting an inaccurately rosy picture.
Identification and Impact: Cherry-picking is closely linked to bias and misleading content. Agents should scrutinize whether an article provides context for the data it cites. If an article presents a statistic, does it mention relevant comparisons or time frames? If not, it might be cherry-picking. For instance, stating “Policy A created 1 million jobs!” sounds impressive, but if that’s over 10 years and in the same period 900,000 jobs were lost due to the policy in another sector, then just saying the million figure alone is cherry-picked. Multimodal clues: Charts and graphs can also be cherry-picked. An article might show a graph that starts at a specific convenient date to highlight growth, omitting earlier data that would show a decline. A classic case is cropping a chart’s timeline or truncating the y-axis (see Statistics section on visuals) to exaggerate a trend. Agents spotting such tricks should flag the content as misleading by omission. The intent behind cherry-picking might be to persuade or deceive (if done knowingly, it borders on disinformation) or could be just the author’s lack of thoroughness. In any case, it undermines completeness and objectivity. Content identified as cherry-picking should be flagged and generally results in a lower content quality score.
Example: A cryptocurrency blog post proclaims “Project Y’s user base grew 300% this quarter,” which sounds tremendous. On investigation, you find the user count went from 100 to 400 (a small base) and last quarter they had lost 50% of users (from 200 down to 100). The blog chose only the rebound quarter to talk about. This is a textbook cherry-pick. An unbiased article would mention both the drop and the subsequent growth for honesty. By explicitly defining cherry-picking, we ensure agents will recognize when content is presenting a one-sided dataset and adjust their trust in the content accordingly.
Completeness / Comprehensive Coverage
Definition: “Completeness” in content refers to how fully and thoroughly a piece of writing covers the topic at hand. A comprehensive article “covers a subject completely or broadly; it is inclusive of all necessary elements”​
merriam-webster.com
. In other words, the article addresses the key questions a reader might have about the topic, provides sufficient background or context, and doesn’t leave significant gaps. A comprehensive article includes relevant details, evidence, and perspectives needed to understand the issue. For example, a complete macroeconomic analysis of an interest rate hike would mention the decision of the central bank, the reasons behind it (inflation data, employment rates), and the potential impacts on markets and consumers. It would not omit a major facet like how this compares to previous rate changes or global context. In a crypto article about a new blockchain protocol, completeness means covering what the protocol is, how it works (at least at a summary level), what problem it solves, and any notable risks or criticisms – rather than just, say, announcing its launch with no explanation. Essentially, completeness is about scope: has the author touched on all the major points readers need?
Importance in Scoring: Completeness is a major quality metric because incomplete information can mislead or leave the reader without understanding. An article lacking completeness might be termed superficial or have fluff in place of substance (see Essential Content vs. Fluff). There’s a difference between brevity and incompleteness: an article can be concise yet comprehensive if it intelligently summarizes all essentials. Agents should evaluate whether an article seems to cover the “5 Ws and H” (Who, What, Where, When, Why, and How) as appropriate. For a news article, completeness might involve answering those questions. For an analytical piece, it might involve exploring causes, effects, and possible solutions or alternatives. In political journalism, completeness means if discussing a policy, the article covers what the policy entails, its intended goal, supporters and critics, and relevant history or data. If an article only presents a policy’s positive side and never mentions criticisms, it’s not providing a full picture (lacking completeness and likely biased). Similarly, a crypto market report that only lists price changes without any context (like news events or historical comparison) is incomplete – it tells what happened but not why or how it fits into the bigger picture.
Relevance and Multimodal Content: Completeness also entails including pertinent facts that give context. For example, providing historical data or definitions of jargon contributes to completeness, especially since our definitions assume readers (and agents) have no prior domain knowledge. Charts, graphs, or infographics can be used to enhance completeness by visualizing data that’s hard to convey in text. A comprehensive macroeconomic article might include a graph of GDP over time to complement the text explanation. However, including a chart alone isn’t enough – the article should explain it. If the content references an external event (say a political scandal) and expects the reader to know it, that might reduce completeness. A truly comprehensive piece would include a brief recap or explanation of that event. Agents should ask: Does this article stand on its own in explaining the topic? If crucial information is missing (forcing the reader to look elsewhere to fill gaps), the article is not fully complete.
When scoring, content that is comprehensive tends to be in higher tiers (e.g. Very Good or above), because it indicates thorough research and informative value. On the contrary, content that is shallow or oversimplified (given its context) will score lower. Note that more words don’t automatically mean completeness; sometimes content can be long but still not cover key points, wandering into tangents instead (that’s fluff). Agents should differentiate between breadth (covering many aspects) and depth (detailed analysis – see Depth definition). The best articles have both breadth and depth: they are complete (breadth) and insightful (depth). In summary, “completeness” ensures no vital piece of the puzzle is missing from the article’s presentation of the topic.
Credible / Trustworthy
Definition: “Credible” content is content that can be believed and trusted. A credible article comes from a place of evidence, expertise, or reliable sources and thus carries authority. Formally, credible means “capable of being believed; worthy of belief or confidence; trustworthy”​
dictionary.com
​
dictionary.com
. In evaluating credibility, agents consider both source credibility and content credibility. Source credibility involves who is providing the information – for instance, an established financial newspaper or a noted expert lends credibility by reputation, whereas an unknown blog requires more scrutiny. Content credibility focuses on what’s in the article itself – does it back up claims with evidence? Does it cite reputable sources or data? Is the tone measured and factual? Does it avoid logical fallacies and extreme bias? A trustworthy article typically includes verifiable facts (with references or links to data/reports), uses a neutral tone, and is written by someone with apparent knowledge of the subject (or at least citing those who have that knowledge). For example, a trustworthy macroeconomics article might reference data from the World Bank or a central bank report when discussing economic growth. A credible crypto article might quote known developers or link to the project’s technical documentation or audit reports. In political content, credibility could come from quoting official statements, referencing laws, or using on-record interviews.
Signs of Credibility: Look for transparency and evidence. Credible content often provides sources for key facts – e.g., “According to a 2022 IMF report, global GDP grew by 6%” (with a citation or at least a clear reference). It avoids sweeping claims without backing. It may also present counterarguments fairly (which paradoxically increases credibility because it shows the author isn’t hiding anything). Credible articles also fact-check their assertions. If an article corrects or acknowledges uncertainties (“Initial reports said X, but this is unconfirmed”), that honesty boosts trustworthiness. Multimodal aspects: Inclusion of primary documents or images of evidence can enhance credibility – for instance, embedding a graph of unemployment from the official statistics office, or a photograph of an event (assuming the photo is authentic and relevant). However, the presence of a chart alone isn’t enough – one must check if the chart itself is from a credible source or if it’s possibly a manipulated graphic. Metadata like the author’s credentials, or if it’s published on an official site (e.g., a government press release), also factor into credibility. A “Verified/Official” source (see Verified/Official Source definition) inherently starts as more credible until proven otherwise.
Trust vs. Truth: It’s important to note that credible does not always guarantee the content is true, but it implies a high likelihood of truth. For example, a major news outlet can make mistakes, but because they have fact-checking processes and a reputation to uphold, their content is generally credible. Conversely, a random social media post making a big claim with no evidence is not credible, even if by fluke the claim turns out true. Agents must gauge credibility by considering bias and intent too – a very biased piece is less credible because it’s likely omitting facts. An article full of sensationalism or hype is less credible because exaggeration often means evidence is lacking. On the other hand, an objective, calm tone tends to be more trustworthy. Intentional deception (disinformation or scam content) is of course utterly non-credible. But even unintentional misinformation reduces credibility if the author didn’t do due diligence.
When scoring, credibility issues push content into lower score ranges. A credible and well-sourced article is essential for a top-tier score. Agents should explicitly mention credibility judgments in their evaluations (e.g., “The article is credible, citing multiple expert sources and data” vs “The article lacks credibility due to anonymous claims and no evidence”). If an article makes bold assertions like “Project Z will revolutionize finance and guarantee profits” without proof, that’s a red flag – not credible. In sum, credibility/trustworthiness is about content reliability. Always ask: “Should a reader trust what this article is saying? Why or why not?” Our defined criteria will guide that answer.
Depth / In-Depth / Technical Depth
Definition: “Depth” refers to the level of detail, sophistication, and insight presented in the content. An in-depth article doesn’t just skim the surface; it delves into the “how” and “why” of the topic, providing thorough explanations or analysis. Technical depth specifically means that the content covers complex, technical aspects with accuracy and clarity, often required for topics in finance, economics, or technology. Depth can be thought of as the article’s vertical thoroughness: whereas completeness is about covering a broad range of points (breadth), depth is about exploring those points in detail. A deep article will examine causes and effects, include nuanced discussion, perhaps go into historical background or technical mechanisms, and anticipate reader questions. For example, a superficial crypto article might say “Blockchain X is innovative and faster than Bitcoin” and leave it at that. A deeper article would explain how Blockchain X achieves faster speeds (e.g., describing its consensus algorithm or architecture), and maybe discuss trade-offs or technical challenges. In macroeconomics, depth means instead of just noting “inflation is up,” an in-depth piece would discuss the underlying drivers (supply chain issues, monetary policy, etc.), possibly break down inflation by sectors, and discuss implications for different groups. In political writing, depth could involve not just reporting a new law, but analyzing how it compares to previous laws, the political context, and potential future consequences.
Why Depth Matters: Depth is a hallmark of high-quality, insightful content. It indicates the author either has expertise or has done significant research. For the reader (and thus for our scoring), a deeper article is more valuable because it teaches more. Depth often involves technical correctness and detail – which means the content must remain accurate while being detailed. One challenge is that going in-depth can introduce jargon or complexity; a truly excellent article will balance depth with clarity, possibly by explaining technical terms (thus remaining accessible). Technical depth is especially relevant in crypto articles (which might discuss algorithms, cryptography, smart contracts) and macroeconomic articles (which could get into the weeds of monetary theory, statistical data, etc.). Agents should assess if the content appropriately matches its expected depth: A beginner-friendly explainer might not be extremely deep by design, but it should still be accurate on fundamentals. A whitepaper or expert analysis, on the other hand, should exhibit a high level of technical depth – if it doesn’t, it might be too shallow for its intended audience.
Interactions with Other Terms: Depth often goes hand in hand with completeness – a deep article likely is comprehensive as well – but not always. It’s possible for an article to be deep on a narrow aspect but ignore other angles (deep but not broad). For example, an article might deeply analyze technical data but not mention broader context, which could confuse readers (this would be deep but not entirely complete). Ideally, top-tier content is both deep and broad enough. Bias can affect depth: a biased source might deliberately omit depth on points that counter their narrative (e.g., not diving into an opposing argument’s merits). Factual accuracy is critical – an article can attempt depth but if the details are wrong, it backfires. Agents should verify that the depth provided is correct and relevant. Also, depth should be relevant: sometimes an article can go on a tangent with technical detail that isn’t actually pertinent to the main point (that would be unnecessary depth that doesn’t add value, sometimes a form of fluff in disguise). Good depth is relevant detail that enriches understanding.
When scoring, content with strong depth usually falls in the upper bands (e.g., Very Good or Excellent), especially for topics where depth is expected. Conversely, content that is shallow or oversimplified (given its context) will score lower. For instance, a political opinion piece that just states an opinion with a couple of slogans but no analysis is shallow – likely low quality. Meanwhile, a piece that thoroughly investigates an issue, cites studies, and dissects arguments shows depth – indicating high quality. Depth also implies that the article likely required more effort and expertise, which should be acknowledged in scoring. Agents may note phrases like “lacks depth,” “superficial treatment,” or “offers in-depth analysis” in their evaluations to justify scores per these definitions.
Disinformation
Definition: “Disinformation” is false information spread deliberately to deceive​
dictionary.com
. Unlike misinformation (which can be accidental), disinformation always involves intent – the person or group sharing it knows it’s false or highly misleading. Dictionary definitions emphasize this distinction: “Disinformation refers to false information that’s spread with the specific intent of misleading or deceiving people”​
dictionary.com
. It is often systematic and can be part of propaganda campaigns. For example, a fabricated news story claiming a political candidate was involved in a scandal, released by an opposition group to sway an election, would be disinformation if the creators knew it was untrue. In crypto, an example could be someone intentionally spreading a rumor that a certain exchange is insolvent to cause panic and profit from short positions – if they know it’s false, that rumor is disinformation. Disinformation is a subset of misinformation (all disinformation is misinformation, but not all misinformation is disinformation)​
dictionary.com
. The key element is malicious intent to deceive.
Characteristics: Disinformation often comes in forms that mimic credible information, which is why it can fool people. It may include doctored images, fake “leaked” documents, or impersonation of legitimate sources. In politics, disinformation might appear as a fake news article or a forged tweet from a public figure. In macroeconomics or finance, it could be bogus “insider info” meant to move markets. Agents should remain vigilant for signs of intentional deception: extremely outrageous claims with no credible source, patterns of a story being pushed by known dubious outlets, or content that appears timed to cause maximum confusion. If a piece of content is identified as disinformation, it is among the worst content on our quality scale – even if superficially well-written – because it’s fundamentally a lie engineered to mislead. Bias and intent: disinformation often aligns with a certain bias (e.g., propaganda to favor or smear a side), but what distinguishes it from mere bias or speculation is that the purveyor knows it’s false. For scoring purposes, agents typically won’t know the internal mindset of the author, but clues can suggest disinformation: if an article cites a “fact” that has been widely debunked by all reliable sources, yet presents it as true, one can suspect disinformation (especially if the outlet has a track record of spreading falsehoods intentionally).
Handling and Impact: When agents encounter likely disinformation, they should categorize the content as extremely low quality (often a score in the 0.x to 1 range, effectively “false/misleading content with malicious intent”). The presence of disinformation triggers the strict rule that content is automatically down-rated to the lowest scores due to harmful falsity. For example, if a crypto article falsely claims “The SEC has approved a Bitcoin ETF” (when in reality it has not) in order to pump the market, that’s disinformation. Even if some parts of the article are well-written, that core false claim ruins its integrity. Agents should document why they believe the information was shared deliberately: maybe the rumor was traced to a source trying to manipulate prices, or the article’s publisher is known for fabrications. Relevance of disinformation: ironically, disinformation content might sometimes include a lot of (false) detail – a fake story might have fabricated quotes, dates, etc. That “depth” is not genuine depth; it’s part of the deceit. So, while at a glance it might look comprehensive, it’s built on falsity. In evaluating, truthfulness overrides all – no amount of detail or completeness can save disinformation from a poor score because the foundation (accuracy) is broken.
Finally, agents should be aware that labeling something disinformation is serious; ensure to differentiate it from misinformation. If unsure about intent, it may be safer to call it misinformation (unintentional) but still score it low for falsehood. Only label disinformation when there’s clear reason to believe the deception is intentional (e.g., known hoax origin, or content is so outlandish and unsubstantiated that it’s hard to imagine it’s a mistake). Either way, both misinformation and disinformation are harmful, but disinformation is the more egregious due to bad faith.
Edge Case
Definition: In our classification context, an “edge case” refers to content that doesn’t clearly fit the normal categories or could be interpreted multiple ways. Essentially, it’s a borderline or unusual instance that tests the limits of our definitions. This could be content that mixes genres (news vs. opinion vs. satire), content that’s ambiguously sarcastic, or topics that straddle domains (like a piece equally about macroeconomics and politics). Edge cases require careful judgment and sometimes additional steps to classify properly. For example, a tweet that is half meme, half news might be an edge case: is it informative or just a joke? An article that is satire but not obviously labeled as such is an edge case because it can be mistaken for misinformation. A macroeconomic article heavily referencing a niche crypto concept might be an edge case in terms of domain crossover, requiring the agent to use both sets of guidelines.
Why Edge Cases Occur: The world of content is diverse, and no guidelines can pre-cover every scenario. New formats or trends emerge (like “DeFAI” – a hypothetical emergent crypto sector combining DeFi and AI). Also, sometimes content creators do unconventional things (like a political op-ed written as a fictional story – mixing fact and fiction). These create situations where the usual classification approach might not directly apply. By calling something an edge case, we flag that it requires extra attention and perhaps a custom approach.
Handling Edge Cases: When an agent encounters an edge case, they should first identify why it’s an edge case (what about it is unusual?). Next, they should refer back to fundamental principles: accuracy, intent, harm. If something is satire, the fundamental accuracy is low but the intent to deceive is also low; the agent should then be careful not to treat it as malicious misinformation. If an article straddles crypto and politics, the agent might need to combine criteria (e.g., treat it as crypto content but also consider political bias aspects). The Addendum at the end of this section provides a step-by-step procedure for agents facing new or undefined terms or scenarios. In short, agents should:
* Research or clarify any unknown concept.

* Apply analogous definitions (e.g., treat “DeFAI” with same scrutiny as any DeFi concept, checking for hype or technical accuracy).

* Not jump to conclusions – edge cases often need a bit of investigation.

* Document their reasoning for how they classified it (so that later reviewers or guideline updates can learn from it).

Edge cases are not inherently good or bad content; they are just tricky. However, because they fall outside the usual patterns, there’s higher risk of misclassification. Our system encourages agents to explicitly mark edge cases and proceed with caution. If needed, they should consult additional resources or ask for human review (depending on the system’s design). By defining “edge case,” we ensure agents have a concept for “this one is unusual; handle with extra care.” This means using the definitions as tools flexibly. For instance, if an edge case piece is both humorous and informative, the agent might decide to classify primarily on its informative aspect but note the humor.
In summary, labeling content an edge case means the agent will take extra steps to clarify how to classify it, often by combining guidelines or seeking clarification. This helps maintain consistency and fairness when novel situations arise.
Essential Content vs. Fluff
Definition: Essential content refers to the information in an article that is substantive, necessary, and directly contributes to the article’s purpose or thesis. Fluff, by contrast, is any content that does not add real value – it’s filler, padding, or superfluous text that could be removed without loss of meaning or clarity. Fluff often manifests as overly verbose descriptions, repetitious statements, off-topic anecdotes, or generic commentary that isn’t backed by facts. One way to define fluff is “content that doesn't add value, engagement, or context to your content.”​
freelancewritingjobs.com
It’s the opposite of concise, meaningful content. For example, in a crypto news article, the essential content would be the facts of the news (what happened, when, who’s involved, implications), whereas fluff might be a long-winded introduction about how “Cryptocurrency has been on a wild ride through the ages of human innovation…” – a flowery tangent that doesn’t actually inform the reader about the news item. In a macroeconomic report, essential content includes the actual data and analysis; fluff could be needless rhetorical questions or over-general history that isn’t directly relevant.
Identifying Fluff: Agents should ask, for each part of an article: Does this serve a clear purpose (inform, explain, argue)? If a paragraph or sentence could be removed and the reader would still understand the topic just as well, it might be fluff. Common forms of fluff:
   * Wordy phrasing: using ten words when five would do, just to sound elaborate. For instance, saying “Due to the fact that” instead of “Because” – these add length but not substance.

   * Redundant statements: repeating the same point in different words without adding new insight.

   * Off-topic personal anecdotes or asides: e.g., a political article where the author spends three paragraphs sharing a personal story that doesn’t shed new light on the political issue.

   * Overly general filler: e.g., a crypto article starting with “Since the dawn of civilization, people have traded value…” – true, but too broad and not specifically useful to the topic of crypto.

   * Empty hype or vague claims: statements that sound grand but don’t actually provide information (e.g., “This development is truly remarkable and one of a kind” with no explanation why).

Fluff is not just about length; even a short article can have fluff if it wastes the reader’s time on non-informative content. Conversely, a long article might be very tight with essential content if every section is informative. Essential content includes the core facts, analysis, insights, and necessary context that fulfill the article’s goal. It’s what the reader came for. For a tutorial or explainer, the step-by-step instructions and clarifications are essential, whereas a long personal story in the middle is likely fluff. In journalism, direct quotes from sources, verified facts, and explanation of significance are essential; a bunch of speculative adjectives or irrelevant background is fluff.
Why it Matters: The presence of fluff can significantly lower an article’s clarity and efficiency. Readers often recognize fluff intuitively – it’s when you start skimming because the text isn’t saying anything new. From a scoring perspective, heavy fluff indicates poorer quality because it suggests either a lack of substance (nothing more to say, so the author is padding) or lack of editing. It can also be misleading in a way – fluff might exaggerate or obscure the lack of actual facts (like smoke and mirrors). For example, a company press release might be full of fluff (“world-leading, innovative, groundbreaking solution…”) but provide no concrete details on what the product does – that lowers credibility.
Agents should reward content that is concise yet comprehensive – meaning it covers what’s needed without unnecessary filler. Articles that get to the point and stay on point are higher quality. On the other hand, if an article feels bloated or meandering, point out the fluff. One might write in an evaluation: “The article contains a lot of fluff – lengthy paragraphs of general commentary that do not contribute facts or analysis. The essential information could have been conveyed in half the space.” That would justify a lower score in terms of coherence and value. Note: Some narrative styles (like feature journalism) allow for storytelling that might seem tangential but ultimately tie back to the main point – agents should distinguish fluff from purposeful narrative elements. Fluff truly has no purpose. If a “storytelling” section provides context or humanizes the issue, it might be essential for engagement even if it’s not raw data – use judgment. But if it’s just there to increase word count or because the author went off on a tangent, it’s fluff. In summary, essential content is what makes the article worthwhile; fluff is what an editor might trim out. Quality content maximizes the former and minimizes the latter.
Fear, Uncertainty, and Doubt (FUD)
Definition: FUD stands for “Fear, Uncertainty, and Doubt.” Calling something “FUD” means it is spreading negative, often exaggerated or baseless, information that instills fear or anxiety in the audience. It is considered a propaganda or manipulation tactic​
en.wikipedia.org
: the goal is to influence perception by disseminating worrisome or dubious information, usually without solid evidence, to make people afraid of or hesitant about something. The term originated in marketing/politics, but in the crypto world it’s commonly used to label content that is overly pessimistic or alarmist about a coin or the market, possibly to drive prices down. For example, a rumor that “Country X is going to ban Bitcoin tomorrow, so everyone should sell now!” might be called FUD if there’s no confirmation and it seems intended to spark panic selling. In politics, FUD might appear as a smear campaign with lines like “If Candidate Y is elected, the economy will collapse” without factual backing – just trying to scare voters. In macroeconomics, an article titled “Is a Great Depression around the corner? Experts say total collapse is imminent” that cherry-picks worst-case opinions and lacks balanced data could be spreading FUD to get clicks or push an agenda.
Recognizing FUD: FUD content typically has a sensationalist tone, emphasizing worst-case scenarios or dangers. It often uses words like “crash,” “collapse,” “doomed,” “danger,” etc., in speculative ways. Importantly, FUD usually is not backed by solid evidence – it might cite anonymous sources or no sources at all for its dire claims. It’s inherently biased toward negative outcomes. This doesn’t mean all warnings are FUD; a critical factor is whether the fear being spread is justified by facts. If an article says “Scientists warn of a severe pandemic unless measures are taken” and this is based on legitimate research, that’s not FUD, that’s a factual warning. But if someone falsely claims “The vaccine will cause widespread harm” with no evidence, that’s FUD (and also misinformation/disinformation). So, intent and factual basis separate legitimate caution from FUD. Many times in crypto communities, any negative news is reflexively called FUD, but as evaluators, we must discern if the negatives are real or concocted.
Impact on Scoring: Content that is pure FUD (i.e., misleading fearmongering) is low-quality because it misleads and often lacks balance and accuracy. If an article is simply negative but based on truth (for example, reporting a real security vulnerability – that might cause fear but it’s factual), it is not to be penalized as FUD. We penalize when the fear content is exaggerated or unfounded. Often, FUD overlaps with misinformation (if false claims are involved) or at least misleading by omission or exaggeration. Agents should check: are the scary claims supported by data or sources? If not, it’s likely FUD. Does the content acknowledge uncertainty, or does it speak in absolutes of doom? Responsible writing will clarify uncertainty; FUD often uses uncertainty as a tool to amplify fear (ironically, the “U” in FUD stands for uncertainty itself). For instance, “No one knows what will happen, but you could lose everything – why take the risk?” That line spreads doubt without solid reasoning.
In terms of scoring, a piece that mainly spreads FUD for sensational effect would score poorly (perhaps in the “Poor” or “Very Poor” range), especially if it’s clear the intent is to scare rather than inform. Even if it doesn’t contain outright falsehoods, the heavy bias and lack of contextual truth make it low quality. If the FUD seems deliberate (like part of a campaign to disinform), it could even be considered disinformation and rated at the very bottom. However, if the content raising fear is actually presenting a legitimate concern with evidence (like a cybersecurity expert listing potential threats in a factual manner), then it’s not FUD in the negative sense, and could be high quality.
In summary, when we say an article is “full of FUD,” we critique it for being exaggerated pessimism without basis. Agents need to articulate this: e.g., “The article is misleadingly alarmist, using FUD tactics: it makes dire predictions of a stock market crash with no credible analysis or sources – thus not trustworthy.” This aligns with our goal to identify attempts at manipulative negativity and rate them accordingly.
Good / High-Quality Content
Definition: “Good” content (often corresponding to a mid-to-high score like 6.1–7.0 on our scale) is solid and reliable, meeting the expectations for accuracy and clarity, though perhaps not exceptional in depth or insight. It is high-quality in the sense that it fulfills its purpose well: the facts are correct, the piece is organized, and it provides value to the reader. However, it might not go above and beyond in the way an excellent or outstanding piece does. To put it simply, a good article gets the job done effectively. If someone asks a direct question and the article answers it with the right information and a clear explanation, that’s good quality. It might not be memorable or extraordinarily detailed, but it’s competently written and trustworthy.
Attributes of Good Content: A good article will be accurate – there should be no significant errors or falsehoods. It will likely be complete enough for the average reader’s needs, though maybe not fully comprehensive for advanced readers. It tends to be balanced and fair, showing at least some attempt at neutrality (no glaring bias). Credibility is present: sources are cited or the information is commonly known facts, and the author isn’t making unsupported claims. The structure is logical: introduction of the topic, main points, supporting details, and a conclusion or summary. The writing style is clear, possibly straightforward; even if it’s not eloquent or deeply engaging, it doesn’t confuse the reader. High-quality content avoids obvious fluff or filler; most of what’s included serves a purpose. It’s also relevant to its topic and up-to-date if discussing current events. For example, a good macroeconomic news piece would accurately report the latest interest rate decision, include a couple of quotes from experts (maybe a central bank official and an economist), and give a bit of context about what that means for markets. It might not analyze 10 years of historical data or alternative scenarios (that would be very good or excellent if it did), but what it says is correct and useful.
Differentiating “Good” from “Very Good” or “Excellent”: Think of “Good” as meeting the criteria. A very good piece might exceed them slightly by adding perhaps one extra layer of analysis or being particularly well-written. An excellent piece exceeds them more significantly by being comprehensive and insightful. So, a good article might have minor omissions – maybe it doesn’t cover every angle, but covers the main ones. It might have minor bias or minor unclear phrasing, but nothing that seriously misleads. For instance, a crypto explainer that correctly covers what a blockchain is and how it works in simple terms would be good; if it also compared different blockchains and gave real-world examples, that might bump it to very good or excellent. If it just hits the basics and stops, it’s still good/high-quality because it achieved its primary goal.
Why Label Content as Good: For agents, calling something “Good” or “high-quality” is a positive judgment indicating the content is in the upper-middle tier. It means you would feel comfortable using or sharing this content as a reliable source, even if you know there might be better or more detailed sources out there. Good content is not perfect—few things are—but any flaws are minor and don’t significantly detract from the value. For example, maybe an article doesn’t go into depth on one subtopic, but that subtopic might be beyond its scope, so it’s forgivably omitted. Or maybe the article has a slight tilt (like it’s generally pro-crypto in tone), but it doesn’t spread misinformation or completely ignore counterpoints, so it remains overall fair. These would still be considered good articles.
In scoring and feedback, agents might say: “This is a high-quality piece – it’s accurate and well-explained, with sufficient examples. It could have been improved by including perspective X or by adding a chart, but as is, it’s reliably informative.” That encapsulates the “good” rating – strong and competent, with perhaps some room for enhancements.
Maintaining this definition helps ensure that not everything is labeled top-tier; we reserve “outstanding” and “excellent” for the truly exceptional, “good” for the reliably strong content, and “fair/average” for content that is okay but with notable gaps.
Inaccurate
Definition: “Inaccurate” content contains information that is incorrect, erroneous, or not true. It is the opposite of accurate. Even if the piece is otherwise well-written, any significant factual error makes parts of it inaccurate. This could be as blatant as a wrong number or statistic, or as subtle as a misquote or a claim that isn’t supported by evidence (and is actually false). For example, if an article on macroeconomics says “Unemployment is currently 2%” when it’s actually 5%, that’s a clear inaccuracy. If a crypto article claims “Ethereum’s supply is 210 million coins” (confusing it with Bitcoin’s 21 million cap), that’s inaccurate. Inaccuracies may arise from mistakes, outdated information, or misunderstanding by the author. Regardless of intent, inaccurate information diminishes content quality because it misleads readers about the facts.
Types of Inaccuracy:
      * Factual errors: concrete things like dates, figures, names that are wrong. (E.g., saying someone was president in 2015 when they weren’t, or citing a $10 billion market cap when it’s $1 billion).

      * Conceptual errors: misunderstandings of concepts leading to false statements (e.g., describing a proof-of-stake system as proof-of-work – an important technical distinction incorrectly conveyed).

      * Misrepresenting sources: maybe an article cites a study but interprets it incorrectly, thus giving inaccurate conclusions about what the study said.

      * Out-of-context facts (which can lead to inaccuracy in effect): e.g., quoting someone out of context such that the meaning is distorted (the quote itself is accurate but taken alone it conveys a false impression).

Contexts: In crypto, inaccuracies often involve technical details or market facts – e.g., misreporting a coin’s max supply or a security vulnerability. In macro articles, an inaccuracy might be mixing up economic indicators or mis-stating cause and effect (like saying a strong currency causes high inflation – generally inaccurate). In political content, inaccuracies might be wrong statements about a candidate’s record or about the content of legislation.
Impact and Scoring: Content with notable inaccuracies is generally pushed into low quality tiers because it fails the basic requirement of informing correctly. If the inaccuracies are few and minor (like a small date error that doesn’t change the overall understanding), the content might still salvage a mediocre score, but if they are core to the content, it could drop even a potentially good piece into the poor range. For instance, an otherwise well-argued article about inflation becomes unreliable if it’s built on an incorrect inflation statistic. One crucial task for agents is to verify suspicious or important facts. If something stands out (“Is that true?”), the agent should, in an ideal workflow, check it. If it’s false, that’s an inaccuracy tag.
It’s also important to consider intent vs. mistake: an inaccuracy could be unintentional misinformation, or if it seems deliberate, it edges into disinformation. But even if unintentional, the content’s effect is to mislead. Agents judge the content, not the author’s mind – so inaccurate content is problematic regardless. The agent might note: “The article is inaccurate regarding X, which misrepresents the situation.” If a piece has multiple inaccuracies or one big one, it severely undermines credibility (see Credible).
Cascading effect: One inaccuracy can throw off completeness and balance too. If a key fact is wrong, the conclusions or coverage might miss the mark. For example, an article analyzing a policy based on an incorrect statistic might thoroughly analyze the wrong data – making the whole thing less useful. So inaccuracies often correlate with other quality issues (though not always – you could have a well-structured article with one glaring error; it’s still a big issue).
In conclusion, any content with clear inaccuracies should be scored significantly lower. Minor inaccuracies should be pointed out and factor into the rating (perhaps taking an article from potentially good down to fair or poor). Major inaccuracies – especially if central to the piece – can render the content effectively “false or misleading,” which on our scale is very low. Our definition here ensures no agent shrugs off a false statement just because other parts are fine; factual correctness is foundational.
Misinformation
Definition: “Misinformation” is false or incorrect information that is spread, regardless of intent to mislead​
dictionary.com
. In other words, it’s any information that is not true, but crucially, the person spreading it might not realize it’s false. The difference from disinformation is the absence of deliberate intent. For example, if someone shares a news article containing an untrue claim (believing it’s true), they are spreading misinformation. A lot of rumors or urban myths are misinformation. In our context, a crypto blog post that shares an untrue claim like “Elon Musk will keynote a Bitcoin conference next week” (when he isn’t) is misinformation if the author heard this rumor and believed it without verification. The information is wrong, but they aren’t intentionally lying – they just didn’t fact-check. Key point: no intent to deceive is required for information to be labeled misinformation​
dictionary.com
. It’s about the content being wrong, not the motive​
dictionary.com
.
Common Sources of Misinformation:
         * Rumors and speculation presented as fact. (E.g., “Exchange X might be insolvent, I heard from someone” published as if true.)

         * Outdated information that has since changed, but is presented as current. (E.g., quoting last year’s law as if it’s still in effect after it was repealed.)

         * Misinterpretations: The author might misunderstand a complex issue and hence provide wrong info (like misreading a research finding).

         * Satire taken seriously: Sometimes satirical content is shared by someone who thinks it’s real – it then becomes misinformation out of context.

         * Simple mistakes: Typos or misstatements that turn a truth into a falsehood (saying “million” instead of “billion” for example).

Handling Misinformation: From an agent’s perspective, misinformation is harmful because it misleads, even if unintentionally. We treat it similarly to other inaccuracies: it lowers the content’s quality. If the piece is largely misinformation (numerous false claims), it will be rated very low (likely in the bottom tiers, 1-2 out of 10). If there’s one piece of misinformation in an otherwise okay article, it’s still a serious flaw – accuracy is key. The agent should flag it: “The article contains misinformation – specifically the claim X is false (as verified by source Y).” The presence of misinformation often overlaps with credibility issues – a credible source usually avoids misinformation by fact-checking. So finding misinformation likely means the content isn’t well-vetted or the author lacked knowledge.
Importantly, agents must be careful not to conflate differing opinions with misinformation. Misinformation is about factual content being wrong. If an article says “In my view, this policy will fail” – that’s an opinion, not a factual claim that can be true/false yet. But if it says “This policy has never been tried before” and in reality it was tried last year, that’s misinformation (a false factual statement).
Multimodal and Source Considerations: Misinformation can come via text or images (like a mislabeled photo). If a chart is fabricated or data is misplotted, that’s visual misinformation. Agents should cross-verify data from charts with their labels if possible. Also consider source: if the content cites a source that is known to be unreliable or the source itself was misinformation, then the content is propagating misinformation (even if the author cited a source, the info is still wrong).
In summary, misinformation tagging and scoring is about catching false content without needing to prove malicious intent. Even well-meaning content that includes false claims is low-quality in terms of information value. Our guideline for agents is: focus on the accuracy of content, not the presumed motive of the author, when dealing with misinformation​
dictionary.com
​
dictionary.com
. And if unsure about a claim, try to verify – if it can’t be verified and seems extraordinary, treat it skeptically.
Misleading
Definition: “Misleading” content is content that may contain elements of truth but is presented in a way that causes the audience to draw a wrong or unjustified conclusion. Something is misleading if it leads readers astray – they come away with an impression that isn’t accurate or fair, even though the content might not be outright false. This often occurs through omission of context, cherry-picked facts, or suggestive phrasing. A classic example: an article shows a one-day stock chart where the market fell and headlines it “Market Crashes!” – technically the data is real (the market did drop that day), but in context the market might be up for the week, so implying a “crash” is misleading. Misleading content isn’t just about data; it can be in the narrative. For example, a political article might mention a politician was present at an event where something bad happened, insinuating involvement, even if the politician had nothing to do with it. The facts (they were present) are true, but the implication is misleading.
Mechanisms of Misleading Content:
            * Lack of context: Stating a fact without the relevant background that makes it meaningful (e.g., “Crime rate doubled!” without noting it doubled from 1 incident to 2 incidents in a small town, which is minimal in absolute terms).

            * Selective presentation: only giving information that supports one narrative (this is essentially cherry-picking – see Cherry-Picking above).

            * Implication and innuendo: wording things to imply something that isn’t explicitly stated. E.g., “The senator met with lobbyists behind closed doors” said in a tone that implies corruption, even if that meeting was a normal part of legislative discussions.

            * Improper correlation/causation: implying A caused B just because A happened before B, without proof (post hoc fallacy).

            * Visual misdirection: a chart with a truncated axis or a photo from an unrelated event used to support a claim can mislead visually.

Intent and Factual Accuracy: Misleading content can be intentional (spin, propaganda) or unintentional (poor explanation). It doesn’t necessarily involve outright false statements, which makes it trickier – it can slip past fact-checkers because the individual pieces are not false. It’s the assembly or emphasis that deceives. Agents must look beyond literal truth to see if the reader might get a wrong idea. For example, an article might list a set of facts that are individually true but arranged in such a way to suggest a false narrative.
Examples in our domains: In crypto, a piece might highlight only successful trades of a strategy and not the losing ones, misleading readers about its profitability. In macroeconomics, an article could say “Government debt reached $X trillion, highest ever!” which is true in nominal terms but misleading if not adjusted for inflation or as a percentage of GDP (which might be average or low historically). In politics, a misleading article might quote a politician out of context: e.g., the politician says “We should not cut education to fix the budget” and the article quotes “We should not cut education” implying the politician is against education cuts in any scenario, warping the meaning to fit a narrative.
Scoring and Treatment: Misleading content is a serious issue because it can deceive without lying. Agents should definitely penalize content identified as misleading. The severity depends on how misleading it is and about what. Mildly misleading (maybe due to slight lack of context) might drop an otherwise good article to fair. Egregiously misleading (clearly crafted to push a false impression) could put it in poor or very poor territory, even if no facts are blatantly false. The agent’s explanation should articulate the misleading aspect: e.g., “This article is misleading because it reports a quarter of data as if it were an annual trend, giving a false impression of growth.” Transparency in the write-up helps the next person (or the content creator, if they see feedback) understand the problem.
Misleading techniques often accompany bias, so you’ll often mark both: e.g., “The biased selection of facts in this piece is misleading.” It may also co-occur with cherry-picking (a subset of misleading by omission) and misleading statistics. So these definitions reinforce each other.
In practice, if an agent is torn on whether something is factual vs. misleading, they should lean on our definitions: literal truth but causing false impression = misleading. An agent example note might be: “While the statements in the article are individually true, the presentation is misleading as it omits key context (Y and Z), leading the reader to believe X, which isn’t true.” That’s how to identify and handle it.
Promotional / Shilling
Definition: “Promotional” content is content primarily designed to advertise, sell, or promote a product, service, brand, or asset. In the context of crypto and finance, the slang term “shilling” refers to aggressively promoting a cryptocurrency or project, often for personal gain, and typically without disclosing one’s vested interest​
babypips.com
. Promotional content often reads more like marketing or PR than journalism or analysis. Its goal is to cast the subject in a purely positive light to attract interest or customers, rather than to inform objectively. For example, a blog post on a crypto site titled “Why Coin ABC Is Guaranteed to Soar – Don’t Miss Out!” that lists only the positives of Coin ABC and urges readers to buy is promotional. If the author owns Coin ABC and doesn’t disclose that, it’s shilling.
Key Indicators:
               * Exaggerated positives and claims: Promotional pieces use glowing language and superlatives (“groundbreaking, revolutionary, highest ever”) without critical evaluation or evidence to support the claims.

               * Lack of balance or negatives: No mention of any downside, risk, or competition. Real informative content usually acknowledges some caveats.

               * Call-to-action language: e.g., “Act now,” “Don’t miss this opportunity,” “Sign up today,” which is common in advertisements.

               * Conflict of interest not disclosed: If we know or suspect the writer has something to gain (like referral links or they represent the project), and they haven’t stated that, it’s shilling.

               * Format and tone: It might sound like a press release or an infomercial. In crypto, they might throw in rocket emojis or phrases like “to the moon” – a clear hype signal.

Quality Issues: Pure promotional content is inherently biased and often misleading by omission (no negatives mentioned). It can also border on misinformation if it makes false promises (“guaranteed returns” – there’s no such thing, and often that’s a hallmark of a scam). The credibility is low because the agenda is to persuade, not to inform neutrally. Now, not all promotion is a scam – a company’s press release about a new product is promotional but might contain factual info. However, it’s still not neutral content. For our classification, unless the user specifically wants promotional material, such content is typically low in the quality spectrum for general consumption because it won’t provide a full picture.
Scoring: If an article is clearly an undisclosed promotion or shill piece, agents should score it low (likely in poor range). If it’s disclosed (like “Sponsored content” label), it’s at least honest about intent, but the content’s objectivity is still low. We might treat it a bit better if it’s very factual despite being sponsored, but usually sponsored content is flattering by design. In crypto, shills are rampant; our agent should be trained to spot the difference between genuine analysis vs. someone pumping their bags. For example, an “analysis” that concludes a token is amazing while citing mostly the token’s marketing materials and ignoring any concerns is effectively promotion.
Examples:
                  * A crypto YouTuber’s transcript posted as an article saying “You must check out Project XYZ, it’s going to be huge” repeatedly – that’s shilling, especially if that YouTuber has a referral or got paid by XYZ.

                  * A macroeconomics blog that reads like a bank’s advertisement for its new mortgage product rather than an unbiased explanation – that’s promotional.

                  * An article on a news site that is actually a paid piece by a company (common in some outlets as “partner content”) pushing that company’s narrative.

Agent approach: Identify if content is meant to sell rather than inform. If yes, mark how it fails to be balanced or credible. E.g., “This article reads as promotional material for X coin – it lists numerous benefits with no evidence and ignores any risks. The tone is hyperbolic and there’s a direct invitation to invest. Thus, it’s not an objective analysis and is low-quality by our standards.” The agent should apply definitions like bias (clearly biased positive) and essential vs fluff (promos often have fluff in the form of marketing jargon). If false claims are present, then also misinformation.
One nuance: an official announcement from a project (like “Company A announces partnership with B”) is inherently promotional, but as long as it sticks to truth and facts, it may be fine for what it is (a primary source). But an agent would note it’s one-sided (company’s perspective only).
In summary, promotional/shilling content is identified by its one-sided praise and persuasive intent, and is rated lower unless the task specifically expects an advertisement. Non-disclosed shilling especially undermines trust and should be called out by the agent in their evaluation.
Reliable Source / Data
Definition: A “reliable source” is an origin of information that is generally recognized as authoritative and accurate​
scribbr.com
. Reliable data refers to data that comes from such trustworthy sources or through rigorous methods, making it dependable. In practical terms, sources like official government statistics, peer-reviewed research, reputable news organizations, and well-established experts are considered reliable. Information from these sources can usually be cross-verified and tends to have undergone some fact-checking or quality control. Conversely, a random forum post, an unknown blog, or a biased think-tank with an agenda would not be considered as reliable without additional corroboration.
Application in Content: When an article uses facts and figures, the credibility of those facts heavily depends on where they came from. For example, if a macroeconomic article states a country’s GDP growth, citing the World Bank or an official government report makes that data reliable. If it cites an anonymous Twitter account for the same, that’s not reliable. A crypto article discussing a project’s security might reference an audit by a known firm (reliable) versus baseless claims on social media (unreliable). An agent should look for attribution like “according to [Source]” and judge that source’s trustworthiness. If no source is given for a contentious fact, that’s a red flag. As our scribbr reference puts it: “A credible source is free from bias and backed up with evidence. It is written by a trustworthy author or organization.”​
scribbr.com
Why It Matters: Use of reliable sources boosts an article’s credibility and accuracy. It shows the author did homework and isn’t just speculating or propagating rumors. On the other hand, reliance on poor sources undermines content. If an article’s evidence is unreliable, the conclusions can’t be trusted. Even if the author is honest, using bad data yields bad info (garbage in, garbage out). We specifically instruct agents to differentiate claims with and without reliable sourcing​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j. For instance, “Studies show…” without naming the study is weaker than “A Harvard study (2023) shows…”. The former is vague; the latter points to a specific presumably reliable source.
Assessing Reliability: Agents don’t have a master list of all reliable sources, but general knowledge and context help. An official report, a major well-known publication, academic journals, or direct data (like blockchain transaction data from a reputable explorer) are reliable. A YouTube comment, an opinionated blog with no references, or a single person’s claim with no evidence is not. If a new or obscure source is cited, agents can consider factors like the source’s transparency, expertise, and reputation.
In-Text Indicators: Does the article provide citations or links for data? Does it quote people by name and title (so one can assess their authority)? For example, quoting the CEO of a crypto exchange about that exchange’s policy is a reliable primary source for that information (they would know their policy). But quoting an anonymous “crypto expert on Telegram” is not reliable.
Scoring Influence: Content that consistently uses reliable sources and data can be scored higher on credibility. Agents should highlight positive use of sources: “The article uses data from NOAA and NASA, lending credibility to its climate analysis.” Conversely, if content makes big claims with no sources or dubious sources, that’s a negative: “The article claims a new vaccine is unsafe citing only a fringe blog – source is not reliable, so the claim is unsubstantiated.” That lowers the score (likely into poor, because lack of reliable evidence for a bold claim is a major flaw).
Additionally, reliable data presentation means not just citing but presenting data accurately (no cherry-picking or misrepresenting what the source said). If an article cites a reliable source but twists its findings, that’s misleading. So agents should ensure the use of the source is faithful.
In summary, a hallmark of high-quality content is foundation on reliable sources/data. Agents will look for explicit sourcing, evaluate those sources’ credibility, and reflect that in the content’s score. Content that stands on personal assertions or unreliable hearsay will be knocked down for low credibility. As a rule of thumb: the more an article can point to evidence outside itself (and the more that evidence is trustworthy), the more reliable and higher quality it is.
Relevance
Definition: “Relevance” refers to how pertinent and useful the information is to the topic and the reader’s needs. An article is relevant if it stays on topic and includes information that directly helps to understand the subject or answer the questions at hand. If content is irrelevant, it either goes off on tangents unrelated to the main subject or includes outdated or extraneous material that doesn’t contribute to the piece’s purpose. In simpler terms, relevant content is “closely connected or appropriate to what is being discussed or considered.”​
mtnviews.org
For example, in a piece about Ethereum’s merge upgrade, relevant details would include how the technology changes, effects on energy usage, and implications for users and investors. Irrelevant details might be a lengthy history of Ethereum’s price movements – interesting, but not directly related to the merge itself.
Scope of Relevance:
                     * Topical relevance: Does each part of the article relate to the main topic or support the main argument? A relevant article won’t introduce a random section that doesn’t tie back. E.g., a macroeconomic outlook article should not suddenly spend paragraphs on an unrelated political scandal.

                     * Temporal relevance (timeliness): In fast-moving fields like crypto or current events in politics, information can become irrelevant if it’s too old. A 2018 statistic cited in a 2025 article as if current is not relevant (unless the context is historical comparison). Agents check if the content is up-to-date for its publishing time. An article about “recent trends” that uses data from five years ago without noting it’s historical fails relevance in terms of timeliness.

                     * Audience relevance: Good content is pitched at the right level for its audience. Overly technical detail in a beginners’ guide might be irrelevant to those readers (though it might be relevant detail for experts). Our scoring focuses more on correctness and completeness, but if something is clearly out-of-place for the intended audience (the content itself often indicates audience, like a tutorial vs. a research paper), the agent can comment on that as a clarity/usability issue.

                     * Extraneous content: This is similar to fluff. If a chunk of text does not contribute to the main point, it’s irrelevant. This might be a personal anecdote that doesn’t illuminate the topic, or a sidebar about something only tangentially related.

Why It Matters: Irrelevant content can distract or confuse readers. It wastes space that could be used for more pertinent info. It might indicate the author didn’t stay focused or is padding content (see Fluff). For a reader seeking knowledge on a topic, irrelevant content is noise that can cause them to miss the signal. From a scoring perspective, relevance ties into coherence and value: a highly relevant article delivers what the title or introduction promises without detours; an irrelevant article might frustrate the reader.
Examples:
                        * A crypto whitepaper summary should not veer into talking about an unrelated project except to directly compare; otherwise it’s irrelevant.

                        * A political news story about a policy shouldn’t spend half the article recounting an only loosely related historical event unless it directly informs understanding of the policy.

                        * An academic paper’s literature review is relevant in that context (it sets stage for research). But in a short news explainer, a deep literature review would be irrelevant.

Agent considerations: Mark content down if there are significant irrelevant sections. For instance, “Paragraphs 4-6 discuss general blockchain history, which, while true, do not inform the reader about the specific topic of the article – this portion is irrelevant.” On the flip side, praise relevance: “The article remains tightly focused on the new law, providing only background directly pertinent to understanding it.”
Relevance often impacts completeness too: an article filled with irrelevant info probably omitted some relevant info due to that wasted space. It also affects clarity: extraneous info can obscure the main message.
In summary, agents should check that each part of the content serves the article’s apparent goals. Off-topic or outdated content should be seen as reducing quality. Highly relevant content gets the point across efficiently and effectively, which is a hallmark of top-tier writing. The user’s time and attention are respected when content is relevant – this aligns with our quality aims, hence we define and enforce consideration of relevance.
Sarcasm / Satire
Definition: Sarcasm and satire are rhetorical techniques where the literal meaning of words is not the intended meaning. Sarcasm is a sharp, often ironic remark meant to mock or convey contempt, frequently by saying the opposite of what one means (“Great job!” said to someone who failed, meaning the job was not great)​
quillbot.com
. Satire is a broader genre that uses humor, irony, exaggeration, or ridicule to expose and criticize stupidity or vice, often in the context of contemporary politics or other topical issues​
litcharts.com
. Satirical content often takes the form of fictional news (like The Onion articles) or caricature to make a point indirectly. Both sarcasm and satire are not meant to be taken at face value. In content classification, identifying sarcasm or satire is important because literal interpretation would misclassify the content as misinformation or malicious when actually the intent is comedic or critical.
Examples:
                           * Sarcasm: In a crypto forum, user writes “Oh, fantastic, another ICO, just what the world needs.” They mean the opposite – they think there are too many ICOs already. If an agent took it literally as praise, they’d misunderstand.

                           * Satire: A well-known satirical site posts an article “Federal Reserve Announces Plan to Buy All Reddit Memes to Combat Inflation.” Obviously not true, but intended to humorously comment on the Fed or meme stocks, etc. Anyone familiar with the site or tone knows it’s a joke; others might mistake it for real if they aren’t careful.

Identification: Sarcasm in text can be hard to detect, as it relies on tone. Look for cues: over-the-top praise in an otherwise negative context, or an obviously absurd statement following a serious setup. Emoticons or tags (like “/s” online) explicitly mark sarcasm sometimes. Satire is easier if it’s from a known satire outlet or clearly fantastical. If the content is extremely implausible but written in the style of a news report, suspect satire. Agents should be aware of known satire sources (The Onion, Babylon Bee, etc.) and also check if the content might be referencing something ironically. Sometimes an article itself may hint it’s satire (a disclaimer, or the category is “Humor”).
How to Treat in Scoring:
                              * If content is clearly satire, it’s intentionally not factual. We shouldn’t rank it as misinformation because its intent isn’t to mislead as true information. However, unless the evaluation is about humor quality (usually not), a satirical piece doesn’t fulfill the role of providing reliable information. So while we don’t condemn it like false news, we would note that it isn’t informative. In some guidelines (like for tweets) we might label it separately. In our scoring, a satirical piece might get a middling score if we’re evaluating from an info perspective (because while not malicious, it has no factual value and could mislead uninformed readers). If the context suggests the reader is meant to know it’s satire, the agent can note that.

                              * If sarcasm is used within an otherwise factual piece (like a sarcastic aside or a sarcastic tone), the agent should interpret the intended meaning correctly when evaluating accuracy or bias. E.g., if an author sarcastically says “Yeah, because bribery is totally legal (eyeroll)” the agent knows the author means bribery is illegal and is criticizing someone who acted like it’s fine. The agent should not flag “the author incorrectly said bribery is legal” because that was sarcasm. They should, however, consider that heavy sarcasm in a supposedly factual article might confuse some readers or indicate bias. Tone-wise, excessive sarcasm might lower clarity or professionalism.

                              * If content could be satire but it’s not clear, the agent might flag it as a possible edge case: “This piece might be satirical due to X and Y clues, but it’s not explicitly stated. If satire, it’s intentionally false for humor; if not, it’s wildly inaccurate.” The agent might then lean on context (publication, author, etc.) to decide.

Policy for Agents: The Tweet guidelines example said not to penalize obvious satire as misinformation. Similarly, here we emphasize understanding intent. Our definition equips agents to not label clearly satirical news as intentionally deceptive. But they still must classify it appropriately (likely as not useful for factual content). For sarcasm in user comments or style, the agent should “decode” it to the real meaning before applying our other definitions like bias or accuracy.
In summary, sarcasm/satire are special cases where literal content doesn’t match actual intent. Agents are expected to recognize these cases (not always easy, but known signals and context help) and avoid misclassifying them as lies or errors. Instead, they note the use of sarcasm/satire and evaluate based on intended meaning and context. This definition ensures all agents share that understanding and treat such content uniformly.
Scam / Scammy
Definition: “Scam” content is content that is part of a fraudulent scheme designed to deceive people into giving up money, sensitive information, or other valuables. If we call content scammy, it means it exhibits characteristics of a scam. Common examples are phishing attempts, fake investment opportunities, Ponzi schemes, advance-fee frauds (“send me $100 and you’ll get $1000 later”), or counterfeit giveaways (“send 1 ETH to this address to receive 2 ETH back”). In crypto, scams include things like fake ICOs, rugpull projects, phishing sites mimicking wallets or exchanges, or posts by impersonators of famous people asking for crypto transfers. In politics or other domains, a scam could be a fake charity plea or any hoax message seeking funds or personal data.
Indicators of Scam Content:
                                 * Unrealistic promises or guarantees: e.g., “Guaranteed 300% return in a week!” (legitimate investments never guarantee such returns).​
file-y5vjotteyhnen3w6bqws3j

                                 * Urgency and pressure: “Act now or miss out!”, “Only a few slots left!”, trying to rush the reader so they don’t have time to think or research.​
file-y5vjotteyhnen3w6bqws3j

                                 * Request for money or personal info: especially upfront payment for something uncertain, or asking for login credentials, private keys, social security numbers, etc. No legitimate article or offer will ask for your password or seed phrase directly.

                                 * Impersonation: content that pretends to be from a trusted source (bank, government, known company or figure) but is not. Look for slight misspellings of brand names, weird sender addresses, or odd URLs.

                                 * Lack of verifiable details: Real opportunities or programs can be checked; scams are often vague about who’s behind them or provide fake contact info.

                                 * Known patterns: e.g., the classic “Nigerian prince” email is a known scam pattern in finance; “double your crypto” giveaways are a known scam in crypto (often using hacked verified Twitter accounts to appear real).​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j

Why Critical: Scam content is malicious and can lead to real harm (financial loss, identity theft). In our scoring, any content identified as a scam should be given the lowest possible trust score (essentially 0 or near it)​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j. The guidelines typically state if something is clearly a scam, you don’t bother weighing accuracy or balance – it’s categorically bad. For example, an investment “article” that’s actually a multi-level marketing recruitment pitch disguised as a success story is a scam. Even if some details in it are factual, the intent is to defraud.
Agent Action: The moment a piece is recognized as scammy, the agent should:
                                    1. Mark it as malicious content.

                                    2. Not trust any of its informational content (since it’s there to deceive).

                                    3. Explain why it’s flagged as a scam using the indicators. For instance: “This content is asking readers to send money to an address to double it – a known crypto scam technique. It promises impossible returns and creates false urgency. It’s clearly scam content.”

                                    4. Score it at the bottom (e.g., 0.1 out of 10). In a classification context, we might not even analyze it further for quality; it’s automatically in the worst category (like how the crypto tweet guide said any scam = very low score).

Examples:
                                       * A fake giveaway on YouTube live streams often show a message: “Send 0.5 BTC to our address and get 1 BTC back! Limited time.” That’s scam content.

                                       * An email or article claiming “The IRS owes you a refund, click here to claim” (phishing for info).

                                       * A supposed news article on a site that’s actually a front for a Ponzi scheme, containing mainly glowing “testimonials” and a sign-up link asking for a fee.

                                       * Any content asking for private keys or recovery phrases – legit sources never do that, so it’s a scam attempt.

Important: Not all promotional content is a scam (some might just be biased marketing). A scam involves deception with intent to steal or defraud. Agents should differentiate: e.g., a biased article pushing a real product is promotional (low quality but not a direct scam). But a fake product or one that takes money and gives nothing is a scam. Context helps: unknown entity, unbelievable claims, no third-party coverage or legitimacy signals -> likely scam.
In conclusion, scam content is to be identified and dealt with harshly in scoring. Our definitions equip agents to spot typical scam signals and understand that normal evaluation criteria (like neutrality or completeness) almost become irrelevant since the whole content is disqualified by its fraudulent nature. Protecting users from scams is a priority, so marking such content clearly as malicious and low-quality is essential.
Statistics (Misleading or Well-Used)
Definition: This entry addresses how numerical data is presented. Misleading statistics are statistics that are presented without proper context or in a distorted way that misrepresents the truth​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j. This can occur through selective use of data, inappropriate comparisons, or graphical tricks. Well-used statistics are the opposite: data presented with full context, accurate scales, and clear explanation, giving an honest representation.
Common ways statistics mislead:
                                          * Omitting context or denominators: e.g., “X company’s profits grew 100%” (but maybe from $1 to $2 – a tiny profit in absolute terms)​
file-y5vjotteyhnen3w6bqws3j
. Without baseline context, the percentage is misleading.

                                          * Using absolute vs. relative measures inappropriately: e.g., reporting “5,000 new cases” when the relevant scope would be per capita or percentage (5,000 might be big or small depending on population).

                                          * Cherry-picked time frame: e.g., showing a stock’s price increase over 2 weeks and calling it a long-term boom, ignoring that over a year it’s down (data selection bias).

                                          * Truncated or stretched axes in graphs: e.g., a bar chart where the y-axis doesn’t start at zero, making small differences look huge​
en.wikipedia.org
. Or using an inconsistent scale.

                                          * Statistical fallacies: e.g., confusing correlation with causation (implying one stat caused another without proof), or using mean vs median inappropriately to skew perception.

                                          * Small sample / anecdotal stats pitched as significant: “Our user satisfaction is 100%” (from a survey of 3 users – not reliable, but that detail is hidden).

Well-used statistics, on the other hand, are characterized by:
                                             * Proper context (e.g., “Crime increased 10% this year, from 100 cases last year to 110 this year, in a city of 1 million – still historically low”).

                                             * Clear sourcing (so the reader can verify the stat).

                                             * Appropriate visualization (graphs with labeled axes, starting points that make sense, and not exaggerating scale).

                                             * Comparisons that are apples-to-apples (e.g., using inflation-adjusted dollars when comparing across decades).

                                             * Honest interpretation (pointing out limitations or margin of error if relevant).

Impact on Perception: Statistics carry weight, so misleading stats can be a powerful way to misinform. They often reinforce bias or hype. For agents, catching misuse of stats is critical. A statement can be factually numeric but contextually misleading. We explicitly teach agents to call out stats that lack context or use a selective frame​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j. Conversely, when an article presents data responsibly, it should be credited for thoroughness.
Examples:
                                                * Crypto: “Token ABC grew 50% this month!” If ABC went from $0.02 to $0.03, that’s 50% but trivial in value – if article doesn’t mention the price base or that it previously dropped, it misleads via stat.

                                                * Macro: A graph of unemployment from 8% to 9% might have a y-axis from 8% to 9% only, making the increase look like a giant jump visually (misleading graph)​
en.wikipedia.org
. A well-used stat graph would maybe show 0–10% axis, so one sees it’s a moderate rise.

                                                * Politics: “Our policy cut poverty by 5 million people.” If population grew, maybe poverty rate stayed same. Without stating rates or context, the stat could mislead (maybe poverty % actually rose).

Agent approach: Evaluate if stats given are supported by source and context. If a stat seems outlandish or used to make a strong point, check if the article contextualizes it. If not, that’s a quality issue (either incomplete or misleading). If a chart is present, the agent should examine axes and labels; many guidelines mention truncated axes as a red flag because readers may be misled​
en.wikipedia.org
. Agents should reward content that provides both raw numbers and percentages, or current vs historical comparisons, as appropriate — this shows completeness and honesty in data usage​file-y5vjotteyhnen3w6bqws3j​file-y5vjotteyhnen3w6bqws3j.
If an article uses a stat incorrectly (e.g., using mean income when median would avoid skew), the agent can mark that as a mistake or misrepresentation.
In scoring, misleading stats contribute to a misleading content assessment, hence a lower score. Well-used stats contribute positively to depth and credibility. We may explicitly note, “The article responsibly explains statistics with context, enhancing clarity” which is a plus.
All in all, agents are trained via this definition to be vigilant with numbers: not to accept them blindly, but to see how they’re used. This ensures that content doesn’t get a free pass for having numbers — it matters how those numbers are presented. We want truthfulness in spirit, not just letter, and statistics can be bent; our definition of misleading vs well-used stats enforces that consideration.
Very Good / Excellent / Outstanding
Definition: These terms denote high tiers of content quality, often used in our scoring rubric to differentiate strong content from the absolute best. They are somewhat subjective, but we have criteria:
                                                   * Very Good content is content that is notably above average – it is accurate, clear, and fairly comprehensive, with only minor areas for improvement. It might correspond to a score roughly in the 7.1–8.0 range (on a 0–10 scale). A very good article covers the topic well and may only miss a bit of depth or have a slight bias or a couple minor omissions. It’s the kind of content one could rely on confidently, even if it’s not exceptional. For example, an article that explains a new technology in detail, cites sources, and is mostly balanced (maybe one minor viewpoint left out) would be very good.

                                                   * Excellent content meets virtually all of our criteria for quality – it’s accurate, comprehensive, well-structured, balanced, and perhaps even insightful or engaging. This might correspond to an 8.1–9.0 score. An excellent piece goes the extra mile: after reading it, the audience’s questions are answered and they have gained nuanced understanding. Only extremely minor quibbles (like perhaps a little too much jargon, or a single unimportant fact not included) separate it from perfection. It stands out as high-quality among peer content. For instance, a political analysis that not only reports facts but connects them insightfully and includes all relevant perspectives could be labeled excellent.

                                                   * Outstanding content is the best of the best, roughly a 9.1–10.0 score​
file-y5vjotteyhnen3w6bqws3j
. It is exceptional in accuracy, depth, clarity, and insight – often bringing something unique like exclusive information, exceptionally clear explanations of complex ideas, or innovative analysis. There is essentially nothing significant to criticize. Outstanding pieces often become reference points that others cite​file-y5vjotteyhnen3w6bqws3j​file-y5vjotteyhnen3w6bqws3j. Think of investigative journalism that wins awards, or an authoritative whitepaper that shapes an industry – that’s outstanding. In our rubric, “Outstanding” content meets all the criteria of excellent content and then exceeds in either breadth, depth, or originality.

Differentiating Factors:
                                                      * Very Good vs. Excellent: A very good article might be thoroughly informative but perhaps not as engaging or not covering one subtopic that an expert might expect. An excellent article would cover that subtopic as well or present the info in a slightly more digestible way or with better visuals, etc. Essentially, excellent is very good + something extra (more polish, a bit more depth or insight).

                                                      * Excellent vs. Outstanding: An excellent article might do everything right, but an outstanding article does everything right and offers a level of depth or perspective that is rare. Outstanding content might have original research or an exceptionally clear way of explaining that others haven’t done. It often has high impact. One can think, “Could this piece serve as the definitive explanation or report on this topic?” If yes, it’s likely outstanding.

Why These Distinctions Matter: In scoring content, especially if using a fine scale, it’s useful to distinguish the good from the truly great. It helps in training models or giving feedback – to push content creators from very good to excellent, for instance, one can identify that one missing element. Likewise, to achieve outstanding, one often needs to do something beyond expectations.
Our guidelines previously created detailed breakdowns for scores 7-10, likely describing these tiers​
file-y5vjotteyhnen3w6bqws3j
​file-y5vjotteyhnen3w6bqws3j. For example:
                                                         * Scores 7.1–8.0: Very Good Content – high quality, maybe one minor issue or just not as thorough as an 8+.

                                                         * Scores 8.1–9.0: Excellent Content – meets essentially all criteria​
file-y5vjotteyhnen3w6bqws3j
.

                                                         * Scores 9.1–10.0: Outstanding Content – exceptional, basically perfect in context​
file-y5vjotteyhnen3w6bqws3j
.

Agents’ Use: When an agent says content is very good, they might note the small gap: e.g., “This article is very good, only lacking a recent data update to be excellent.” For excellent, “This article is excellent: comprehensive, neutral, and well-sourced.” For outstanding, “This piece is outstanding – it not only covers everything with precision but also provides original insights and is written brilliantly clearly. I can find no substantive faults.”
These terms set a benchmark. They ensure not every decent piece gets labeled “excellent” – that’s reserved for the top tier. Meanwhile, “good” (previously defined) covers the solid, and below that would be fair/average or worse.
In summary:
                                                            * Very Good = High quality, just shy of excellence (small improvements possible).

                                                            * Excellent = Virtually complete and correct in all aspects (top few percent).

                                                            * Outstanding = The best one could reasonably expect; exemplar of quality, perhaps even exceeding normal expectations with unique value.

By defining them together, we give agents a clear hierarchy and qualitative difference to look for. It eliminates ambiguity between someone’s “8/10” and “9/10” – now they have descriptors. This consistency yields more reliable scoring and feedback.
Addendum: Procedure for New or Undefined Terms
The information landscape in crypto, macroeconomics, and politics evolves rapidly. New technologies, jargon, or trends (e.g., an emergent crypto sector like “DeFAI”) may appear that our current definitions don’t cover. Agents might encounter terms or content scenarios not explicitly in these guidelines. This addendum outlines how agents should handle such cases to maintain classification accuracy and update our system’s knowledge:
1. Identify the Unknown: When an agent comes across an unfamiliar term or an ambiguous scenario, they should first acknowledge it. For example, if an article mentions “DeFAI” and the agent hasn’t seen this term before, they note it as potentially undefined. Recognizing “I don’t know this” is crucial to avoid guessing wrong. Sometimes the article itself will define the new term – the agent should look for that (many good articles introduce new concepts by explanation). If the article defines it, then the agent can use that definition.
2. Research Briefly (if possible): Agents should attempt to find a quick, authoritative explanation of the term. They might use internal knowledge bases or reputable external sources. For instance, to figure out “DeFAI,” an agent might recall that “DeFi” is decentralized finance and guess “AI” refers to artificial intelligence, thus DeFAI likely means decentralized finance augmented by AI. If a quick search in a trusted source or the article’s context confirms this, great. If the agent platform allows, a fast lookup in, say, a crypto glossary or Wikipedia can resolve many new terms. The key is to ensure the information source for this research is reliable (to avoid introducing misinformation at the agent level).
3. Use Context Clues: If external research isn’t available, agents rely on the article’s context. Often authors introduce a new term with context. For example: “DeFAI platforms have surged in popularity, offering autonomous lending via AI-driven smart contracts.” From that sentence, even if “DeFAI” was unknown, the agent gleans it’s something about AI in DeFi. Context might give enough to apply relevant definitions (like we know it’s a type of platform, so issues might involve technical depth, etc.).
4. Apply Analogous Criteria: Once the agent has a working idea of the new term, they should classify content involving it using analogous existing definitions. In essence, treat the new concept by the principles we use for similar ones. For DeFAI, the agent would apply all criteria we use for crypto project content: Is it explained clearly (Depth/Comprehensiveness)? Is the discussion hyped (Hype) or balanced? Are claims about AI in finance backed by data (Credibility/Accuracy) or is it speculative? Are there any misleading uses of AI jargon? Essentially, we don’t need a special rule for “DeFAI” if our general rules for accuracy, bias, etc., cover it. We just slot it into the correct domain. If “DeFAI” is a crypto-finance hybrid term, maybe both crypto and macroeconomic criteria apply. Agents have to use judgment, but the core ideas (truth, fairness, completeness) remain the same.
5. Document the New Term: The agent should flag the encounter of a new term in their evaluation notes or feedback, suggesting it be added to the guidelines in the future. For example, they might add, “(Note: ‘DeFAI’ refers to decentralized finance with AI; consider adding to glossary.)” This helps the guidelines maintainers update our definitions set so the next agent won’t face the same uncertainty. This way the classification system learns and adapts over time.
6. Avoid Over-penalizing due to Novelty: If the content itself does a good job explaining the new concept, the agent should not mark it down just because the agent didn’t know the term beforehand. In fact, if after reading the article the agent understands the term, that means the article succeeded in introducing it – that’s a positive (good depth and clarity). Conversely, if the article throws around an undefined new buzzword and never explains it, that’s a clarity/completeness issue to mention. But that issue is on the content, not on the term existing.
7. Escalation for Extreme Cases: In rare edge cases where a concept is so new or complex that even after steps 2-4 the agent is unsure how to apply criteria, the agent can treat it as an edge case (as defined above). They might need to consult a human expert or simply do their best and attach a caveat. E.g., “This content involves a very new protocol. I have evaluated it per our standards (accuracy, bias, etc.), but the novelty means some judgment calls. It appears factual based on available info, but long-term significance is unclear.” The system designers might review such cases separately and refine guidelines accordingly.
8. Example - Handling “DeFAI”: Suppose an agent encounters an article praising “DeFAI”. The agent doesn’t know the term. They see context that it’s about AI in decentralized finance. They recall our definitions of Hype and Bias. They notice the article uses a lot of futuristic promises about DeFAI with little concrete detail – classic hype. The agent, even without a formal “DeFAI” entry, can classify the article as high on hype, maybe low on depth if no technical details given, and possibly lacking credible evidence. They would score it accordingly, citing these issues. Additionally, they note for maintainers that “DeFAI = DeFi + AI, was heavily hyped in this article, suggest adding to glossary.”
By following this procedure, agents ensure that new terms don’t derail the classification process. Instead, they apply our robust existing criteria to new phenomena, ensuring consistent judgments. Simultaneously, the guidelines expand over time to include those new terms, keeping our system up-to-date. In essence: unknowns are addressed through research and analogy, not guesswork, and then turned into knowns by updating our definitions. This proactive approach is critical in fast-evolving fields like crypto and tech.